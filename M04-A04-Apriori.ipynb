{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 - Assignment Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by including the functions to generate frequent itemsets (via the Apriori algorithm) and resulting association rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (c) 2016 Everaldo Aguiar & Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n",
    "#\n",
    "# Functions to compute and extract association rules from a given frequent itemset \n",
    "# generated by the Apriori algorithm.\n",
    "#\n",
    "# The Apriori algorithm is defined by Agrawal and Srikant in:\n",
    "# Fast algorithms for mining association rules\n",
    "# Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(filename):\n",
    "    '''Loads an example of market basket transactions from a provided csv file.\n",
    "\n",
    "    Returns: A list (database) of lists (transactions). Each element of a transaction is \n",
    "    an item.\n",
    "    '''\n",
    "\n",
    "    with open(filename,'r') as dest_f:\n",
    "        data_iter = csv.reader(dest_f, delimiter = ',', quotechar = '\"')\n",
    "        data = [data for data in data_iter]\n",
    "        data_array = np.asarray(data)\n",
    "        \n",
    "    return data_array\n",
    "\n",
    "def apriori(dataset, min_support=0.5, verbose=False):\n",
    "    \"\"\"Implements the Apriori algorithm.\n",
    "\n",
    "    The Apriori algorithm will iteratively generate new candidate \n",
    "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
    "    iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate \n",
    "        candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] R. Agrawal, R. Srikant, \"Fast Algorithms for Mining Association \n",
    "           Rules\", 1994.\n",
    "\n",
    "    \"\"\"\n",
    "    C1 = create_candidates(dataset)\n",
    "    D = list(map(set, dataset))\n",
    "    F1, support_data = support_prune(D, C1, min_support, verbose=False) # prune candidate 1-itemsets\n",
    "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
    "    k = 2 # the itemset cardinality\n",
    "    while (len(F[k - 2]) > 0):\n",
    "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
    "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
    "        support_data.update(supK) # update the support counts to reflect pruning\n",
    "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
    "        k += 1\n",
    "\n",
    "    if verbose:\n",
    "        # Print a list of all the frequent itemsets.\n",
    "        for kset in F:\n",
    "            for item in kset:\n",
    "                print(\"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
    "\n",
    "    return F, support_data\n",
    "\n",
    "def create_candidates(dataset, verbose=False):\n",
    "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
    "    immutable and hashable).\n",
    "    \"\"\"\n",
    "    c1 = [] # list of all items in the database of transactions\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "\n",
    "    if verbose:\n",
    "        # Print a list of all the candidate items.\n",
    "        print(\"\" \\\n",
    "            + \"{\" \\\n",
    "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
    "            + \"}\")\n",
    "\n",
    "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
    "    return list(map(frozenset, c1))\n",
    "\n",
    "def support_prune(dataset, candidates, min_support, verbose=False):\n",
    "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
    "\n",
    "    By the apriori principle, if an itemset is frequent, then all of its \n",
    "    subsets must also be frequent. As a result, we can perform support-based \n",
    "    pruning to systematically control the exponential growth of candidate \n",
    "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
    "    pruned from the input list of itemsets (dataset).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    candidates : frozenset\n",
    "        The list of candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "    \"\"\"\n",
    "    sscnt = {} # set for support counts\n",
    "    for tid in dataset:\n",
    "        for can in candidates:\n",
    "            if can.issubset(tid):\n",
    "                sscnt.setdefault(can, 0)\n",
    "                sscnt[can] += 1\n",
    "\n",
    "    num_items = float(len(dataset)) # total number of transactions in the dataset\n",
    "    retlist = [] # array for unpruned itemsets\n",
    "    support_data = {} # set for support data for corresponding itemsets\n",
    "    for key in sscnt:\n",
    "        # Calculate the support of itemset key.\n",
    "        support = sscnt[key] / num_items\n",
    "        if support >= min_support:\n",
    "            retlist.insert(0, key)\n",
    "        support_data[key] = support\n",
    "\n",
    "    # Print a list of the pruned itemsets.\n",
    "    if verbose:\n",
    "        for kset in retlist:\n",
    "            for item in kset:\n",
    "                print(\"{\" + str(item) + \"}\")\n",
    "        print(\"\")\n",
    "        for key in sscnt:\n",
    "            print(\"\" \\\n",
    "                + \"{\" \\\n",
    "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
    "                + \"}\" \\\n",
    "                + \":  sup = \" + str(support_data[key]))\n",
    "\n",
    "    return retlist, support_data\n",
    "\n",
    "def apriori_gen(freq_sets, k):\n",
    "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
    "\n",
    "    This operation generates new candidate k-itemsets based on the frequent \n",
    "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
    "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
    "    items are identical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_sets : list\n",
    "        The list of frequent (k-1)-itemsets.\n",
    "\n",
    "    k : integer\n",
    "        The cardinality of the current itemsets being evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of merged frequent itemsets.\n",
    "    \"\"\"\n",
    "    retList = [] # list of merged frequent itemsets\n",
    "    lenLk = len(freq_sets) # number of frequent itemsets\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk):\n",
    "            a=list(freq_sets[i])\n",
    "            b=list(freq_sets[j])\n",
    "            a.sort()\n",
    "            b.sort()\n",
    "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
    "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
    "\n",
    "            if F1 == F2: # if the first k-2 items are identical\n",
    "                # Merge the frequent itemsets.\n",
    "                retList.append(freq_sets[i] | freq_sets[j])\n",
    "\n",
    "    return retList\n",
    "\n",
    "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
    "    \"\"\"Generates a set of candidate rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "    m = len(H[0])\n",
    "    if m == 1:\n",
    "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
    "    if (len(freq_set) > (m+1)):\n",
    "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
    "        Hmp1 = calc_confidence(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
    "        if len(Hmp1) > 1:\n",
    "            # If there are candidate rules above the minimum confidence \n",
    "            # threshold, recurse on the list of these candidate rules.\n",
    "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
    "\n",
    "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
    "    \"\"\"Evaluates the generated rules.\n",
    "\n",
    "    One measurement for quantifying the goodness of association rules is \n",
    "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
    "    the support for P and H divided by the support for P \n",
    "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
    "    (thus P|H means all the items in set P or in set H).\n",
    "\n",
    "    To calculate the confidence, we iterate through the frequent itemsets and \n",
    "    associated support data. For each frequent itemset, we divide the support \n",
    "    of the itemset by the support of the antecedent (left-hand-side of the \n",
    "    rule).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pruned_H : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
    "    for conseq in H: # iterate over the frequent itemsets\n",
    "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
    "        if conf >= min_confidence:\n",
    "            rules.append((freq_set - conseq, conseq, conf))\n",
    "            pruned_H.append(conseq)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \" ---> \" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
    "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
    "\n",
    "    return pruned_H\n",
    "\n",
    "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
    "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
    "\n",
    "    For each frequent itemset, we calculate the confidence of using a\n",
    "    particular item as the rule consequent (right-hand-side of the rule). By \n",
    "    testing and merging the remaining rules, we recursively create a list of \n",
    "    pruned rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : list\n",
    "        A list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The corresponding support data for the frequent itemsets (L).\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rules : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for i in range(1, len(F)):\n",
    "        for freq_set in F[i]:\n",
    "            H1 = [frozenset([itemset]) for itemset in freq_set]\n",
    "            if (i > 1):\n",
    "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
    "            else:\n",
    "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To load our dataset of grocery transactions, use the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('M04-A02 - groceries data set.csv')\n",
    "D = list(map(set, dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _dataset_ is now a ndarray containing each of the 9835 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9835,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tropical fruit', 'yogurt', 'coffee']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _D_ Contains that dataset in a set format (which excludes duplicated items and sorts them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'citrus fruit', 'margarine', 'ready soups', 'semi-finished bread'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Complete the assignment below by making use of the provided funtions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Instant food products, UHT-milk, abrasive cleaner, artif. sweetener, baby cosmetics, baby food, bags, baking powder, bathroom cleaner, beef, berries, beverages, bottled beer, bottled water, brandy, brown bread, butter, butter milk, cake bar, candles, candy, canned beer, canned fish, canned fruit, canned vegetables, cat food, cereals, chewing gum, chicken, chocolate, chocolate marshmallow, citrus fruit, cleaner, cling film/bags, cocoa drinks, coffee, condensed milk, cooking chocolate, cookware, cream, cream cheese , curd, curd cheese, decalcifier, dental care, dessert, detergent, dish cleaner, dishes, dog food, domestic eggs, female sanitary products, finished products, fish, flour, flower (seeds), flower soil/fertilizer, frankfurter, frozen chicken, frozen dessert, frozen fish, frozen fruits, frozen meals, frozen potato products, frozen vegetables, fruit/vegetable juice, grapes, hair spray, ham, hamburger meat, hard cheese, herbs, honey, house keeping products, hygiene articles, ice cream, instant coffee, jam, ketchup, kitchen towels, kitchen utensil, light bulbs, liqueur, liquor, liquor (appetizer), liver loaf, long life bakery product, make up remover, male cosmetics, margarine, mayonnaise, meat, meat spreads, misc. beverages, mustard, napkins, newspapers, nut snack, nuts/prunes, oil, onions, organic products, organic sausage, other vegetables, packaged fruit/vegetables, pasta, pastry, pet care, photo/film, pickled vegetables, pip fruit, popcorn, pork, pot plants, potato products, preservation products, processed cheese, prosecco, pudding powder, ready soups, red/blush wine, rice, roll products , rolls/buns, root vegetables, rubbing alcohol, rum, salad dressing, salt, salty snack, sauces, sausage, seasonal products, semi-finished bread, shopping bags, skin care, sliced cheese, snack products, soap, soda, soft cheese, softener, sound storage medium, soups, sparkling wine, specialty bar, specialty cheese, specialty chocolate, specialty fat, specialty vegetables, spices, spread cheese, sugar, sweet spreads, syrup, tea, tidbits, toilet cleaner, tropical fruit, turkey, vinegar, waffles, whipped/sour cream, whisky, white bread, white wine, whole milk, yogurt, zwieback}\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate itemsets.\n",
    "C1 = create_candidates(dataset, verbose=True) # candidate 1-itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can prune the candidate - 1 itemsets based on the minimum support criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{whole milk}\n",
      "\n",
      "{frankfurter}:  sup = 0.058973055414336555\n",
      "{long life bakery product}:  sup = 0.037417386883579054\n",
      "{canned fruit}:  sup = 0.003253685815963396\n",
      "{sauces}:  sup = 0.005490594814438231\n",
      "{UHT-milk}:  sup = 0.03345195729537367\n",
      "{soda}:  sup = 0.17437722419928825\n",
      "{organic products}:  sup = 0.001626842907981698\n",
      "{male cosmetics}:  sup = 0.004575495678698526\n",
      "{seasonal products}:  sup = 0.014234875444839857\n",
      "{snack products}:  sup = 0.003050330452465684\n",
      "{bags}:  sup = 0.0004067107269954245\n",
      "{roll products}:  sup = 0.010269445856634469\n",
      "{white wine}:  sup = 0.019013726487036097\n",
      "{herbs}:  sup = 0.01626842907981698\n",
      "{popcorn}:  sup = 0.007219115404168785\n",
      "{cling film/bags}:  sup = 0.011387900355871887\n",
      "{rolls/buns}:  sup = 0.18393492628368074\n",
      "{dish cleaner}:  sup = 0.01047280122013218\n",
      "{flower (seeds)}:  sup = 0.010371123538383325\n",
      "{hygiene articles}:  sup = 0.03294356888662939\n",
      "{female sanitary products}:  sup = 0.006100660904931368\n",
      "{vinegar}:  sup = 0.006507371631926792\n",
      "{onions}:  sup = 0.031011692933401117\n",
      "{semi-finished bread}:  sup = 0.017691916624300967\n",
      "{sausage}:  sup = 0.09395017793594305\n",
      "{salad dressing}:  sup = 0.000813421453990849\n",
      "{fish}:  sup = 0.0029486527707168276\n",
      "{tidbits}:  sup = 0.002338586680223691\n",
      "{honey}:  sup = 0.001525165226232842\n",
      "{liver loaf}:  sup = 0.005083884087442806\n",
      "{napkins}:  sup = 0.05236400610066091\n",
      "{chicken}:  sup = 0.04290798169801729\n",
      "{packaged fruit/vegetables}:  sup = 0.013014743263853584\n",
      "{chewing gum}:  sup = 0.021047280122013217\n",
      "{frozen potato products}:  sup = 0.008439247585155059\n",
      "{prosecco}:  sup = 0.0020335536349771225\n",
      "{chocolate}:  sup = 0.04961870869344179\n",
      "{candles}:  sup = 0.008947635993899338\n",
      "{curd cheese}:  sup = 0.005083884087442806\n",
      "{whipped/sour cream}:  sup = 0.07168276563294357\n",
      "{hair spray}:  sup = 0.0011184544992374173\n",
      "{frozen fruits}:  sup = 0.0012201321809862736\n",
      "{make up remover}:  sup = 0.000813421453990849\n",
      "{sound storage medium}:  sup = 0.00010167768174885612\n",
      "{baby cosmetics}:  sup = 0.0006100660904931368\n",
      "{turkey}:  sup = 0.00813421453990849\n",
      "{curd}:  sup = 0.05327910523640061\n",
      "{mustard}:  sup = 0.011997966446365024\n",
      "{tea}:  sup = 0.003863751906456533\n",
      "{finished products}:  sup = 0.006507371631926792\n",
      "{bottled beer}:  sup = 0.08052872394509406\n",
      "{canned vegetables}:  sup = 0.010777834265378749\n",
      "{ketchup}:  sup = 0.004270462633451958\n",
      "{meat}:  sup = 0.025826131164209457\n",
      "{specialty chocolate}:  sup = 0.03040162684290798\n",
      "{tropical fruit}:  sup = 0.10493136756481952\n",
      "{frozen fish}:  sup = 0.011692933401118455\n",
      "{instant coffee}:  sup = 0.007422470767666497\n",
      "{frozen meals}:  sup = 0.02836807320793086\n",
      "{jam}:  sup = 0.005388917132689374\n",
      "{soap}:  sup = 0.0026436197254702592\n",
      "{mayonnaise}:  sup = 0.009150991357397052\n",
      "{specialty vegetables}:  sup = 0.0017285205897305542\n",
      "{oil}:  sup = 0.02806304016268429\n",
      "{ice cream}:  sup = 0.025012709710218607\n",
      "{pudding powder}:  sup = 0.002338586680223691\n",
      "{dessert}:  sup = 0.03711235383833249\n",
      "{zwieback}:  sup = 0.006914082358922217\n",
      "{canned fish}:  sup = 0.015048296898830707\n",
      "{flour}:  sup = 0.017386883579054397\n",
      "{pot plants}:  sup = 0.01728520589730554\n",
      "{candy}:  sup = 0.0298932384341637\n",
      "{organic sausage}:  sup = 0.0022369089984748346\n",
      "{artif. sweetener}:  sup = 0.003253685815963396\n",
      "{soft cheese}:  sup = 0.01708185053380783\n",
      "{fruit/vegetable juice}:  sup = 0.0722928317234367\n",
      "{processed cheese}:  sup = 0.016573462125063547\n",
      "{detergent}:  sup = 0.019217081850533807\n",
      "{liqueur}:  sup = 0.0009150991357397051\n",
      "{dog food}:  sup = 0.008540925266903915\n",
      "{cake bar}:  sup = 0.013218098627351297\n",
      "{cocoa drinks}:  sup = 0.0022369089984748346\n",
      "{rum}:  sup = 0.004473817996949669\n",
      "{root vegetables}:  sup = 0.10899847483477376\n",
      "{Instant food products}:  sup = 0.008032536858159633\n",
      "{domestic eggs}:  sup = 0.06344687341128623\n",
      "{liquor (appetizer)}:  sup = 0.007930859176410779\n",
      "{softener}:  sup = 0.005490594814438231\n",
      "{salty snack}:  sup = 0.03782409761057448\n",
      "{dishes}:  sup = 0.01759023894255211\n",
      "{dental care}:  sup = 0.005795627859684799\n",
      "{beverages}:  sup = 0.026029486527707167\n",
      "{sugar}:  sup = 0.03385866802236909\n",
      "{bathroom cleaner}:  sup = 0.0027452974072191155\n",
      "{skin care}:  sup = 0.0035587188612099642\n",
      "{chocolate marshmallow}:  sup = 0.009049313675648195\n",
      "{cream cheese}:  sup = 0.03965429588205389\n",
      "{condensed milk}:  sup = 0.010269445856634469\n",
      "{pickled vegetables}:  sup = 0.017895271987798677\n",
      "{sliced cheese}:  sup = 0.024504321301474327\n",
      "{syrup}:  sup = 0.003253685815963396\n",
      "{canned beer}:  sup = 0.07768174885612608\n",
      "{potato products}:  sup = 0.0028469750889679717\n",
      "{cookware}:  sup = 0.0027452974072191155\n",
      "{cleaner}:  sup = 0.005083884087442806\n",
      "{margarine}:  sup = 0.05856634468734113\n",
      "{frozen vegetables}:  sup = 0.04809354346720895\n",
      "{waffles}:  sup = 0.038434163701067614\n",
      "{butter milk}:  sup = 0.027961362480935434\n",
      "{pastry}:  sup = 0.08896797153024912\n",
      "{white bread}:  sup = 0.042094560244026434\n",
      "{cream}:  sup = 0.0013218098627351296\n",
      "{coffee}:  sup = 0.05805795627859685\n",
      "{photo/film}:  sup = 0.009252669039145907\n",
      "{butter}:  sup = 0.05541433655312659\n",
      "{toilet cleaner}:  sup = 0.0007117437722419929\n",
      "{brown bread}:  sup = 0.06487036095577021\n",
      "{nuts/prunes}:  sup = 0.003355363497712252\n",
      "{specialty fat}:  sup = 0.0036603965429588205\n",
      "{cooking chocolate}:  sup = 0.002541942043721403\n",
      "{sweet spreads}:  sup = 0.009049313675648195\n",
      "{decalcifier}:  sup = 0.001525165226232842\n",
      "{cat food}:  sup = 0.023284189120488054\n",
      "{whisky}:  sup = 0.000813421453990849\n",
      "{baby food}:  sup = 0.00010167768174885612\n",
      "{berries}:  sup = 0.033248601931875954\n",
      "{pork}:  sup = 0.05765124555160142\n",
      "{citrus fruit}:  sup = 0.08276563294356888\n",
      "{hamburger meat}:  sup = 0.033248601931875954\n",
      "{misc. beverages}:  sup = 0.02836807320793086\n",
      "{frozen chicken}:  sup = 0.0006100660904931368\n",
      "{brandy}:  sup = 0.004168784951703101\n",
      "{pet care}:  sup = 0.00945602440264362\n",
      "{abrasive cleaner}:  sup = 0.0035587188612099642\n",
      "{preservation products}:  sup = 0.00020335536349771224\n",
      "{specialty cheese}:  sup = 0.008540925266903915\n",
      "{house keeping products}:  sup = 0.008337569903406202\n",
      "{soups}:  sup = 0.00681240467717336\n",
      "{cereals}:  sup = 0.0056939501779359435\n",
      "{yogurt}:  sup = 0.13950177935943062\n",
      "{baking powder}:  sup = 0.017691916624300967\n",
      "{ham}:  sup = 0.026029486527707167\n",
      "{flower soil/fertilizer}:  sup = 0.0019318759532282665\n",
      "{kitchen utensil}:  sup = 0.0004067107269954245\n",
      "{shopping bags}:  sup = 0.09852567361464158\n",
      "{grapes}:  sup = 0.022369089984748347\n",
      "{ready soups}:  sup = 0.0018301982714794102\n",
      "{whole milk}:  sup = 0.25551601423487547\n",
      "{light bulbs}:  sup = 0.004168784951703101\n",
      "{rubbing alcohol}:  sup = 0.0010167768174885613\n",
      "{other vegetables}:  sup = 0.1934926283680732\n",
      "{rice}:  sup = 0.007625826131164209\n",
      "{kitchen towels}:  sup = 0.005998983223182512\n",
      "{frozen dessert}:  sup = 0.010777834265378749\n",
      "{red/blush wine}:  sup = 0.019217081850533807\n",
      "{hard cheese}:  sup = 0.024504321301474327\n",
      "{meat spreads}:  sup = 0.004270462633451958\n",
      "{sparkling wine}:  sup = 0.005592272496187087\n",
      "{pip fruit}:  sup = 0.07564819522114896\n",
      "{pasta}:  sup = 0.015048296898830707\n",
      "{beef}:  sup = 0.05246568378240976\n",
      "{salt}:  sup = 0.010777834265378749\n",
      "{spread cheese}:  sup = 0.011184544992374174\n",
      "{specialty bar}:  sup = 0.027351296390442297\n",
      "{liquor}:  sup = 0.011082867310625319\n",
      "{bottled water}:  sup = 0.11052364006100661\n",
      "{spices}:  sup = 0.005185561769191663\n",
      "{newspapers}:  sup = 0.07981698017285206\n",
      "{nut snack}:  sup = 0.00315200813421454\n"
     ]
    }
   ],
   "source": [
    "F1, support_data = support_prune(D, C1, 0.2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all the frequent itemsets using Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{newspapers}:  sup = 0.08\n",
      "{bottled water}:  sup = 0.111\n",
      "{beef}:  sup = 0.052\n",
      "{pip fruit}:  sup = 0.076\n",
      "{other vegetables}:  sup = 0.193\n",
      "{whole milk}:  sup = 0.256\n",
      "{shopping bags}:  sup = 0.099\n",
      "{yogurt}:  sup = 0.14\n",
      "{citrus fruit}:  sup = 0.083\n",
      "{pork}:  sup = 0.058\n",
      "{brown bread}:  sup = 0.065\n",
      "{butter}:  sup = 0.055\n",
      "{coffee}:  sup = 0.058\n",
      "{pastry}:  sup = 0.089\n",
      "{margarine}:  sup = 0.059\n",
      "{canned beer}:  sup = 0.078\n",
      "{domestic eggs}:  sup = 0.063\n",
      "{root vegetables}:  sup = 0.109\n",
      "{fruit/vegetable juice}:  sup = 0.072\n",
      "{tropical fruit}:  sup = 0.105\n",
      "{bottled beer}:  sup = 0.081\n",
      "{curd}:  sup = 0.053\n",
      "{whipped/sour cream}:  sup = 0.072\n",
      "{napkins}:  sup = 0.052\n",
      "{sausage}:  sup = 0.094\n",
      "{rolls/buns}:  sup = 0.184\n",
      "{soda}:  sup = 0.174\n",
      "{frankfurter}:  sup = 0.059\n",
      "{whole milk, other vegetables}:  sup = 0.075\n",
      "{whole milk, yogurt}:  sup = 0.056\n",
      "{whole milk, rolls/buns}:  sup = 0.057\n"
     ]
    }
   ],
   "source": [
    "F, support_data = apriori(dataset, min_support=0.05, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the association rules from a list of frequent itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{yogurt} ---> {whole milk}:  conf = 0.402, sup = 0.056\n"
     ]
    }
   ],
   "source": [
    "H = generate_rules(F, support_data, min_confidence=0.4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above that setting the minimum support to 0.05 and minimum confidence to 0.4 results in one association and it uses a lot of space for storing the 1-itemset candidates which is expensive."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
